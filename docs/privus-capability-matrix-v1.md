\# PRIVUS Capability Matrix v1.0  

Six-Stage Privacy Capability Model by Elytra Security



This document provides the \*\*Level × Domain\*\* capability view of PRIVUS.



---



\## 1. Levels



P – Preliminary  

R – Responsive  

I – Integrated  

V – Validated  

U – Unified  

S – Strategic  



Maturity deepens from \*\*P → S\*\*, expanding like vision beams from an eye.



---



\## 2. Domains



1\. Governance  

2\. RoPA \& Data Mapping  

3\. Consent  

4\. DSR Handling  

5\. Security \& Telemetry  

6\. Vendor Oversight  

7\. Retention \& Deletion  

8\. Privacy-by-Design \& Engineering Alignment  



---



\## 3. Capability Matrix (v1.0)



| Domain | PRELIMINARY (P) | RESPONSIVE (R) | INTEGRATED (I) | VALIDATED (V) | UNIFIED (U) | STRATEGIC (S) |

|--------|------------------|-----------------|----------------|----------------|-------------|----------------|

| \*\*1. Governance\*\* | No formal privacy governance. Roles unclear. Policies missing or outdated. | Basic policies drafted. DPO or equivalent informally identified. Compliance triggered mainly by incidents or audits. | Privacy governance framework defined and approved. Roles and responsibilities documented and communicated across functions. | Governance practices periodically reviewed and audited. DPO formally empowered. Privacy risks tracked and reported. | Governance integrated into enterprise risk management. Cross-functional privacy council active with regular agenda and decisions. | Privacy is a strategic pillar. Board-level oversight, formal KPIs, and external transparency on privacy posture and improvements. |

| \*\*2. RoPA \& Data Mapping\*\* | No structured RoPA. Fragmented understanding of where personal data resides. | Initial inventory of systems and uses started. Some key processing activities documented. | RoPA completed for core activities and systems. Data flow diagrams and owners identified. | RoPA validated through interviews, system checks, and sampling. Regular review cycle defined (e.g. annually or after major changes). | RoPA integrated with asset management, change management, and security tooling. Updates triggered by new projects or system changes. | Predictive and near real-time understanding of data flows. RoPA integrated into architecture and engineering governance with automation. |

| \*\*3. Consent Management\*\* | Consent (where applicable) captured inconsistently, often without clear records. Notices generic or incomplete. | Basic consent notices updated. Some consent records available for key channels. Withdrawal handled manually. | Consent lifecycle defined and implemented: obtain, record, link to purpose, and withdraw. Templates aligned to DPDPA requirements. | Consent logs periodically reviewed. Version-controlled notices. Alignment with RoPA, purposes, and data uses verified. | Centralised consent and preference management. Integration with CRM, product, and marketing systems. Automations in place for enforcement. | Advanced analytics on consent and preferences. Dynamic, contextual consent for higher-risk activities. Used to drive trust and product design. |

| \*\*4. DSR Handling (Access, Correction, Erasure, Grievance)\*\* | No formalised process. Requests handled ad-hoc, if at all. | Basic SOP drafted. Requests can be logged, usually via email or service desk. Response times vary. | End-to-end DSR workflow with defined steps, owners, and timelines. Identity verification and approval steps in place. | DSR performance monitored. SLA adherence tracked. Sampling of closed cases to verify completeness and correctness across systems. | Central dashboard for DSR metrics and trends. Automated retrieval from key systems. Integrated with grievance mechanisms and escalation paths. | Predictive readiness for DSR volume and complexity. Regular scenario testing. Harmonised global approach (GDPR / DPDPA / other laws) with consistent outcomes. |

| \*\*5. Security \& Telemetry\*\* | Security controls basic or uneven. Logs fragmented or not retained. Limited linkage to privacy. | Endpoint/network/app logs collected in some areas. Incidents addressed manually. Limited breach readiness. | Centralised logging and monitoring. Defined incident response process. Telemetry includes systems handling personal data. | Telemetry coverage validated for critical processing activities. Breach playbooks aligned to privacy impact and notification expectations. | Automated correlation for privacy-relevant events (excess access, anomalous queries, data exfil signals). MXDR or equivalent in place for key environments. | Real-time privacy risk visibility. Integration with threat intelligence and business impact analysis. Continuous control testing and privacy-specific detection logic. |

| \*\*6. Vendor Oversight\*\* | Vendor privacy and security practices largely unmanaged. Contracts may lack data protection clauses. | Basic vendor list compiled. Some DPAs or addendums introduced for new or major vendors. | Vendor register maintained with risk categories (e.g. critical, high, medium). Standard contractual clauses and DPAs adopted. | Periodic assessments or questionnaires for higher-risk vendors. Evidence-based review of key controls and breach handling. | Integrated vendor lifecycle: onboarding, due diligence, contracting, periodic review, and exit. Privacy requirements built into procurement and legal workflows. | Continuous vendor risk and performance monitoring. Data-driven decisions on renewals and onboarding. Strategic alignment on privacy posture with key partners. |

| \*\*7. Retention \& Deletion\*\* | No defined retention periods. Personal data accumulated without planned deletion. | High-level retention schedule drafted for major data categories. Some manual clean-up initiatives. | Retention periods implemented in core systems. Deletion or anonymisation routines scheduled. Logs maintained for key actions. | Retention and deletion practices tested via sampling. Immutable logs (e.g. WORM) available where necessary. Gaps tracked and remediated. | Coordinated retention enforcement across systems and vendors. Central view of retention exceptions and legal holds. | Ongoing optimisation of retention to reduce risk and cost. Minimisation built into design. Retention metrics used in risk and cost discussions. |

| \*\*8. Privacy-by-Design \& Engineering Alignment\*\* | Privacy rarely considered in design. Engineering teams not systematically engaged on privacy requirements. | Basic checklists or guidelines created. Some projects request privacy inputs informally. | PbD integrated into SDLC and project lifecycle (intake, design, testing). Privacy requirements defined per project type. | Formal privacy reviews and sign-offs for major changes. Threat modelling and impact considerations included for personal data processing. | Cross-functional design authority involving privacy, security, legal, and architecture. Repeatable patterns and reference architectures for compliant designs. | Privacy-aware engineering culture. Automated guardrails (linting, templates, patterns). Early-stage product and feature ideation uses privacy as a design driver. |



---



\## 4. How to use this matrix



Typical uses:



\- \*\*Self-assessment\*\*: rate each domain from P to S based on current reality.  

\- \*\*Gap analysis\*\*: identify where capability is stuck (e.g. Governance at I, but Telemetry at P).  

\- \*\*Roadmapping\*\*: define target levels per domain and time horizon.  

\- \*\*Communication\*\*: explain to leadership where you stand and what “better” looks like.  



For formal assessments, evidence models, scoring, heatmaps, and implementation roadmaps, Elytra Security provides a \*\*licensed PRIVUS assessment toolkit\*\* as part of its consulting and product portfolio.



Contact: \*\*partnerships@elytrasecurity.com\*\*



---



\_End of document\_



